{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 14 Exercises - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to learn about a text analysis method called *Named Entity Recognition* (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install spaCy and Download Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install spaCy and the English-language model (`en_core_web_sm`). This is the model that was trained on the annotated \"OntoNotes\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m‚ö† Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.0 MB 9.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=e424438c97353fe4dcf67a70cc18890f3f96a7c44214d2deda3d1cf4277a681d\n",
      "  Stored in directory: /private/var/folders/t1/1xbnlp5j163cd9mt_ht253cw0000gp/T/pip-ephem-wheel-cache-a5clfe1x/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "/Users/melwalsh/opt/anaconda3/lib/python3.8/site-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies ‚Äî such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to import `spacy` and `displacy`, a special spaCy module for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the language model and save it as the variable `nlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we open and read a text file. Then we run `nlp()` on the text to create our processed spaCy document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Harry-Potter-Sorcerer's-Stone.txt\"\n",
    "text = #Open and read the text file\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ‚Äù%‚Äú.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "displacy.render(document, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the named entities in our `document` can be found in the `document.ents` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "document.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing). To get the entity label, we can use the `.label_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    print(named_entity, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"PERSON\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a DataFrame from Python collections like lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = ['Harry', 'Ron', 'Snape', 'Harry']\n",
    "Counter(sample_list).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(sample_list).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Counter(sample_list).most_common(), columns=['character', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get People"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will extract named entities from the text, count them up with `Counter()`, and then create a DataFrame of the results.\n",
    "\n",
    "Your job is to make sure that we only get named entities that are **people**. Insert the approriate Python code below.\n",
    "\n",
    "*Hint: Look at the examples above for guidance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "\n",
    "for named_entity in document.ents:\n",
    "    # Your Code Here\n",
    "        #Your Code Here\n",
    "\n",
    "people_tally = Counter(people)\n",
    "\n",
    "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will extract named entities from the text, count them up with `Counter()`, and then create a DataFrame of the results.\n",
    "\n",
    "Your job is to make sure that we only get named entities that are *either* **countries, cities, states** OR **non-GPE locations**. Insert the appropriate Python code below.\n",
    "\n",
    "*Hint: Look at the examples above for guidance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = []\n",
    "\n",
    "for named_entity in document.ents:\n",
    "    # Your Code Here\n",
    "        # Your Code Here\n",
    "\n",
    "places_tally = #Your code here\n",
    "\n",
    "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|LANGUAGE|Any named language.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will extract named entities from the text, count them up with `Counter()`, and then create a DataFrame of the results.\n",
    "\n",
    "Your job is to make sure that we only get named entities that are **languages**. Insert the approriate Python code below.\n",
    "\n",
    "*Hint: Look at the examples above for guidance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "languages = []\n",
    "\n",
    "for named_entity in document.ents:\n",
    "    # Your Code Here\n",
    "        # Your Code Here\n",
    "\n",
    "languages_tally = Counter(languages)\n",
    "\n",
    "df = pd.DataFrame(languages_tally.most_common(), #Your code here)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Another Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ‚Äù%‚Äú.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the examples above, make a DataFrame of a different named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# Your Code Here üëá\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How well does spaCy's NER seem to be performing?\n",
    "- What does it do well or not so well?\n",
    "- How could you imagine researchers or data scientists using NER?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy vs BookNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare spaCy to BookNLP! Click here: https://colab.research.google.com/drive/1lYezboblkBlf_zPuZ7Kootw5dSvP0tOT#scrollTo=k5oJL-a01kUq\n",
    "\n",
    "BookNLP documentation: https://people.ischool.berkeley.edu/~dbamman/booknlp/README.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

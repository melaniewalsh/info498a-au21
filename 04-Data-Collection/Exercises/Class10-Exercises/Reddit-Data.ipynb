{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Collection and Analysis with PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect Reddit data, we're going to use the [Pushift API](https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/), specifically a Python wrapper for the Pushshift API called [PSAW](https://github.com/dmarx/psaw) (PushShift API Wrapper). Why are we using the Pushshift API instead of the official Reddit API, and PSAW instead of Pushshift itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as Pushshift's creator Jason Baumgartner and his co-authors describe it in their [published paper](https://arxiv.org/pdf/2001.08435.pdf), \"Pushshift makes it\n",
    "much easier for researchers to query and retrieve historical Reddit data, provides extended functionality by providing fulltext search against comments and submissions, and has larger single query limits.\" PSAW, meanwhile, makes it easier to work with Pushshift and provides better documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use PSAW, we first need to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install psaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will import pandas for eventually working with the collected data, and we will change pandas default display setting to make our DataFrame columns wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the PushshiftAPI from psaw and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "\n",
    "# Initialize PushShift\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Reddit Posts (By Subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect Reddit posts, we will use `api.search_submissions()` and then establish the parameters of our query, such as which subreddit we want to search in and what threshold of upvote score we want to set.\n",
    "\n",
    "Below we are setting up to search for posts in the subreddit \"AmITheAsshole\" that have an upvote score of at least 2,000 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_request_generator = api.search_submissions(subreddit='AmITheAsshole',\n",
    "                                               score = \">2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this generator is set up, we can use it to collect Reddit posts. The code below is a list comprehension that loops through the generator and extracts relevant data for each matching Reddit post. It then turns that list into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The cell above should take a while to run. It's searching through Reddit's entire history. It's ok if you periodically get errors while it's running.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick peak of the data, we can look at 10 random rows of data, and only for the columns \"title\" and upvote \"score.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_submissions[['title', 'score']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Another Subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to collect Reddit posts from a different subreddit. Pick a new subreddit and insert it below, then run the cells below to collect Reddit data.\n",
    "\n",
    "*If you need help picking a subreddit, check the [right side bar of the Reddit home page](https://www.reddit.com/) or the [top growing communities](https://www.reddit.com/subreddits/leaderboard/).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_request_generator = api.search_submissions(subreddit='#Your Choice Here',\n",
    "                                               score = \">2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this generator is set up, we can use it to collect Reddit posts. The code below is a list comprehension that loops through the generator and extracts relevant data for each matching Reddit post. It then turns that list into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df  = pd.DataFrame([submission.d_ for submission in api_request_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The cell above should take a while to run. It's searching through Reddit's entire history. It's ok if you periodically get errors while it's running.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the data to examine the top 10 posts in this subreddit with the highest upvote scores. Only examine the columns \"title,\" \"score,\" and \"selftext.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Reddit Posts (By Keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top subreddits for a given keyword? To search by a keyword, you can add `q=` and insert a query phrase. Pick a query phrase and search for it across Reddit by running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_request_generator = api.search_submissions(q='#Your Choice Here'', score = \">2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this generator is set up, we can use it to collect Reddit posts. The code below is a list comprehension that loops through the generator and extracts relevant data for each matching Reddit post. It then turns that list into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame([submission.d_ for submission in api_request_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The cell above should take a while to run. It's searching through Reddit's entire history. It's ok if you periodically get errors while it's running.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a plot of the top 5 subreddits for the keyword that you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
